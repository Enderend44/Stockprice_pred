2025-02-16 22:06:26.746494: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-02-16 22:06:26.772106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-02-16 22:06:26.772137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-02-16 22:06:26.772846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-16 22:06:26.776837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-02-16 22:06:27.322816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Chargement et normalisation des données d'entraînement...
Chargement et normalisation des données de test...
Epoch 1/100, Train Loss: 0.2561, Test Loss: 0.1368
New best model saved with test loss: 0.1368
Epoch 2/100, Train Loss: 0.0818, Test Loss: 0.0196
New best model saved with test loss: 0.0196
Epoch 3/100, Train Loss: 0.0345, Test Loss: 0.0289
Epoch 4/100, Train Loss: 0.0288, Test Loss: 0.0186
New best model saved with test loss: 0.0186
Epoch 5/100, Train Loss: 0.0182, Test Loss: 0.0014
New best model saved with test loss: 0.0014
Epoch 6/100, Train Loss: 0.0125, Test Loss: 0.0007
New best model saved with test loss: 0.0007
Epoch 7/100, Train Loss: 0.0092, Test Loss: 0.0002
New best model saved with test loss: 0.0002
Epoch 8/100, Train Loss: 0.0082, Test Loss: 0.0009
Epoch 9/100, Train Loss: 0.0074, Test Loss: 0.0003
Epoch 10/100, Train Loss: 0.0068, Test Loss: 0.0000
New best model saved with test loss: 0.0000
Epoch 11/100, Train Loss: 0.0089, Test Loss: 0.0010
Epoch 12/100, Train Loss: 0.0070, Test Loss: 0.0018
Epoch 13/100, Train Loss: 0.0057, Test Loss: 0.0006
Epoch 14/100, Train Loss: 0.0055, Test Loss: 0.0002
Epoch 15/100, Train Loss: 0.0050, Test Loss: 0.0001
Epoch 16/100, Train Loss: 0.0055, Test Loss: 0.0017
Epoch 17/100, Train Loss: 0.0051, Test Loss: 0.0000
Epoch 18/100, Train Loss: 0.0044, Test Loss: 0.0000
New best model saved with test loss: 0.0000
Epoch 19/100, Train Loss: 0.0046, Test Loss: 0.0006
Epoch 20/100, Train Loss: 0.0041, Test Loss: 0.0007
Epoch 21/100, Train Loss: 0.0037, Test Loss: 0.0001
Epoch 22/100, Train Loss: 0.0043, Test Loss: 0.0005
Epoch 23/100, Train Loss: 0.0034, Test Loss: 0.0005
Epoch 24/100, Train Loss: 0.0038, Test Loss: 0.0006
Epoch 25/100, Train Loss: 0.0040, Test Loss: 0.0001
Epoch 26/100, Train Loss: 0.0036, Test Loss: 0.0023
Epoch 27/100, Train Loss: 0.0044, Test Loss: 0.0006
Epoch 28/100, Train Loss: 0.0048, Test Loss: 0.0028
Epoch 29/100, Train Loss: 0.0050, Test Loss: 0.0020
Epoch 00029: reducing learning rate of group 0 to 5.0000e-04.
Epoch 30/100, Train Loss: 0.0032, Test Loss: 0.0003
Epoch 31/100, Train Loss: 0.0028, Test Loss: 0.0000
New best model saved with test loss: 0.0000
Epoch 32/100, Train Loss: 0.0030, Test Loss: 0.0007
Epoch 33/100, Train Loss: 0.0029, Test Loss: 0.0007
Epoch 34/100, Train Loss: 0.0027, Test Loss: 0.0015
Epoch 35/100, Train Loss: 0.0033, Test Loss: 0.0003
Epoch 36/100, Train Loss: 0.0034, Test Loss: 0.0016
Epoch 37/100, Train Loss: 0.0026, Test Loss: 0.0008
Epoch 38/100, Train Loss: 0.0028, Test Loss: 0.0008
Epoch 39/100, Train Loss: 0.0026, Test Loss: 0.0001
Epoch 40/100, Train Loss: 0.0023, Test Loss: 0.0005
Epoch 41/100, Train Loss: 0.0024, Test Loss: 0.0002
Epoch 42/100, Train Loss: 0.0025, Test Loss: 0.0010
Epoch 43/100, Train Loss: 0.0025, Test Loss: 0.0005
Epoch 44/100, Train Loss: 0.0025, Test Loss: 0.0005
Epoch 45/100, Train Loss: 0.0024, Test Loss: 0.0007
Epoch 46/100, Train Loss: 0.0024, Test Loss: 0.0002
Epoch 00046: reducing learning rate of group 0 to 2.5000e-04.
Epoch 47/100, Train Loss: 0.0025, Test Loss: 0.0010
Epoch 48/100, Train Loss: 0.0022, Test Loss: 0.0003
Epoch 49/100, Train Loss: 0.0021, Test Loss: 0.0002
Epoch 50/100, Train Loss: 0.0022, Test Loss: 0.0001
Epoch 51/100, Train Loss: 0.0025, Test Loss: 0.0025
Epoch 52/100, Train Loss: 0.0022, Test Loss: 0.0004
Epoch 53/100, Train Loss: 0.0023, Test Loss: 0.0004
Epoch 54/100, Train Loss: 0.0021, Test Loss: 0.0003
Epoch 55/100, Train Loss: 0.0020, Test Loss: 0.0001
Epoch 56/100, Train Loss: 0.0022, Test Loss: 0.0001
Epoch 57/100, Train Loss: 0.0020, Test Loss: 0.0001
Epoch 58/100, Train Loss: 0.0021, Test Loss: 0.0001
Epoch 59/100, Train Loss: 0.0019, Test Loss: 0.0006
Epoch 60/100, Train Loss: 0.0021, Test Loss: 0.0005
Epoch 61/100, Train Loss: 0.0023, Test Loss: 0.0006
Epoch 62/100, Train Loss: 0.0019, Test Loss: 0.0001
Epoch 63/100, Train Loss: 0.0021, Test Loss: 0.0011
Epoch 64/100, Train Loss: 0.0020, Test Loss: 0.0002
Epoch 65/100, Train Loss: 0.0021, Test Loss: 0.0003
Epoch 00065: reducing learning rate of group 0 to 1.2500e-04.
Epoch 66/100, Train Loss: 0.0020, Test Loss: 0.0001
Epoch 67/100, Train Loss: 0.0020, Test Loss: 0.0005
Epoch 68/100, Train Loss: 0.0019, Test Loss: 0.0011
Epoch 69/100, Train Loss: 0.0020, Test Loss: 0.0002
Epoch 70/100, Train Loss: 0.0020, Test Loss: 0.0002
Epoch 71/100, Train Loss: 0.0018, Test Loss: 0.0001
Epoch 72/100, Train Loss: 0.0020, Test Loss: 0.0001
Epoch 73/100, Train Loss: 0.0022, Test Loss: 0.0001
Epoch 74/100, Train Loss: 0.0019, Test Loss: 0.0000
Epoch 75/100, Train Loss: 0.0018, Test Loss: 0.0001
Epoch 76/100, Train Loss: 0.0018, Test Loss: 0.0002
Epoch 77/100, Train Loss: 0.0017, Test Loss: 0.0000
Epoch 78/100, Train Loss: 0.0018, Test Loss: 0.0002
Epoch 79/100, Train Loss: 0.0017, Test Loss: 0.0007
Epoch 80/100, Train Loss: 0.0019, Test Loss: 0.0006
Epoch 81/100, Train Loss: 0.0017, Test Loss: 0.0003
Epoch 82/100, Train Loss: 0.0018, Test Loss: 0.0003
Epoch 83/100, Train Loss: 0.0018, Test Loss: 0.0000
Epoch 00083: reducing learning rate of group 0 to 6.2500e-05.
Epoch 84/100, Train Loss: 0.0017, Test Loss: 0.0002
Epoch 85/100, Train Loss: 0.0018, Test Loss: 0.0001
Epoch 86/100, Train Loss: 0.0017, Test Loss: 0.0003
Epoch 87/100, Train Loss: 0.0017, Test Loss: 0.0003
Epoch 88/100, Train Loss: 0.0016, Test Loss: 0.0002
Epoch 89/100, Train Loss: 0.0018, Test Loss: 0.0001
Epoch 90/100, Train Loss: 0.0017, Test Loss: 0.0002
Epoch 91/100, Train Loss: 0.0018, Test Loss: 0.0005
Epoch 92/100, Train Loss: 0.0018, Test Loss: 0.0003
Epoch 93/100, Train Loss: 0.0017, Test Loss: 0.0002
Epoch 94/100, Train Loss: 0.0018, Test Loss: 0.0005
Epoch 00094: reducing learning rate of group 0 to 3.1250e-05.
Epoch 95/100, Train Loss: 0.0017, Test Loss: 0.0004
Epoch 96/100, Train Loss: 0.0017, Test Loss: 0.0003
Epoch 97/100, Train Loss: 0.0017, Test Loss: 0.0005
Epoch 98/100, Train Loss: 0.0018, Test Loss: 0.0003
Epoch 99/100, Train Loss: 0.0016, Test Loss: 0.0003
Epoch 100/100, Train Loss: 0.0018, Test Loss: 0.0001
Epoch 00100: reducing learning rate of group 0 to 1.5625e-05.
